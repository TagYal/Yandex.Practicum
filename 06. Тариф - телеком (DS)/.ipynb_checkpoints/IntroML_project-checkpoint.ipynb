{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goal\n",
    "Develop a model with the highest possible accuracy. In this project, the threshold for accuracy is 0.75. Check the accuracy using the test dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project instructions\n",
    "**Phase I**\n",
    "1. [ ] Open and look through the data file. Path to the file:datasets/users_behavior.csv Download dataset\n",
    "<br> **Phase II** <br>\n",
    "2. [ ] Split the source data into a training set, a validation set, and a test set.\n",
    "<br> **Phase III** <br>\n",
    "3. [ ] Investigate the quality of different models by changing hyperparameters. Briefly describe the findings of the study.\n",
    "4. [ ] Check the quality of the model using the test set.\n",
    "<br> **Phase IV** <br>\n",
    "5. [ ] Additional task: sanity check the model. This data is more complex than what you’re used to working with, so it's not an easy task. We'll take a closer look at it later.\n",
    "\n",
    "**PHASE V (additional, not in task)** Comparison of results of different model <br>\n",
    "**PostScriptum** Additional attemption to improve model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project evaluation\n",
    "We’ve put together the evaluation criteria for the project. Read this carefully before moving on to the task. <br>\n",
    "Here’s what the reviewers will look at when reviewing your project:\n",
    "- [ ] How did you look into data after downloading?\n",
    "- [ ] Have you correctly split the data into train, validation, and test sets?\n",
    "- [ ] How have you chosen the sets' sizes?\n",
    "- [ ] Did you evaluate the quality of the models correctly?\n",
    "- [ ] What models and hyperparameters did you use?\n",
    "- [ ] What are your findings?\n",
    "- [ ] Did you test the models correctly?\n",
    "- [ ] What is your accuracy score?\n",
    "- [ ] Have you stuck to the project structure and kept the code neat?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PHASE I. Open and look through the data file. Path to the file:datasets/users_behavior.csv Download dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data to dataframe\n",
    "df = pd.read_csv('/datasets/users_behavior.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>calls</th>\n",
       "      <th>minutes</th>\n",
       "      <th>messages</th>\n",
       "      <th>mb_used</th>\n",
       "      <th>is_ultra</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>311.90</td>\n",
       "      <td>83.0</td>\n",
       "      <td>19915.42</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>85.0</td>\n",
       "      <td>516.75</td>\n",
       "      <td>56.0</td>\n",
       "      <td>22696.96</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>77.0</td>\n",
       "      <td>467.66</td>\n",
       "      <td>86.0</td>\n",
       "      <td>21060.45</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>106.0</td>\n",
       "      <td>745.53</td>\n",
       "      <td>81.0</td>\n",
       "      <td>8437.39</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>66.0</td>\n",
       "      <td>418.74</td>\n",
       "      <td>1.0</td>\n",
       "      <td>14502.75</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   calls  minutes  messages   mb_used  is_ultra\n",
       "0   40.0   311.90      83.0  19915.42         0\n",
       "1   85.0   516.75      56.0  22696.96         0\n",
       "2   77.0   467.66      86.0  21060.45         0\n",
       "3  106.0   745.53      81.0   8437.39         1\n",
       "4   66.0   418.74       1.0  14502.75         0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check if the dataframe has been read correctly\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3214 entries, 0 to 3213\n",
      "Data columns (total 5 columns):\n",
      "calls       3214 non-null float64\n",
      "minutes     3214 non-null float64\n",
      "messages    3214 non-null float64\n",
      "mb_used     3214 non-null float64\n",
      "is_ultra    3214 non-null int64\n",
      "dtypes: float64(4), int64(1)\n",
      "memory usage: 125.7 KB\n"
     ]
    }
   ],
   "source": [
    "# according to the describtion of the problem there are no any problems with data. It is a good practice to check it.\n",
    "# check if there are N/A values (), type of data\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**So...** <br>\n",
    "no N/A, type is numerical. *call* and *messages* are not integer but it is not a problem in fact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>calls</th>\n",
       "      <th>minutes</th>\n",
       "      <th>messages</th>\n",
       "      <th>mb_used</th>\n",
       "      <th>is_ultra</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>count</td>\n",
       "      <td>3214.000000</td>\n",
       "      <td>3214.000000</td>\n",
       "      <td>3214.000000</td>\n",
       "      <td>3214.000000</td>\n",
       "      <td>3214.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>mean</td>\n",
       "      <td>63.038892</td>\n",
       "      <td>438.208787</td>\n",
       "      <td>38.281269</td>\n",
       "      <td>17207.673836</td>\n",
       "      <td>0.306472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>std</td>\n",
       "      <td>33.236368</td>\n",
       "      <td>234.569872</td>\n",
       "      <td>36.148326</td>\n",
       "      <td>7570.968246</td>\n",
       "      <td>0.461100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>min</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25%</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>274.575000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>12491.902500</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50%</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>430.600000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>16943.235000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75%</td>\n",
       "      <td>82.000000</td>\n",
       "      <td>571.927500</td>\n",
       "      <td>57.000000</td>\n",
       "      <td>21424.700000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>max</td>\n",
       "      <td>244.000000</td>\n",
       "      <td>1632.060000</td>\n",
       "      <td>224.000000</td>\n",
       "      <td>49745.730000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             calls      minutes     messages       mb_used     is_ultra\n",
       "count  3214.000000  3214.000000  3214.000000   3214.000000  3214.000000\n",
       "mean     63.038892   438.208787    38.281269  17207.673836     0.306472\n",
       "std      33.236368   234.569872    36.148326   7570.968246     0.461100\n",
       "min       0.000000     0.000000     0.000000      0.000000     0.000000\n",
       "25%      40.000000   274.575000     9.000000  12491.902500     0.000000\n",
       "50%      62.000000   430.600000    30.000000  16943.235000     0.000000\n",
       "75%      82.000000   571.927500    57.000000  21424.700000     1.000000\n",
       "max     244.000000  1632.060000   224.000000  49745.730000     1.000000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# more information about data\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**So...** <br>\n",
    "All data are positive as it should be. <br>\n",
    "Max *calls* (244) >> mean (63) + 3 * std (33) = 162 <br>\n",
    "Max *minutes* (1632) >> mean (438) + 3 * std (234) = 1146 <br>\n",
    "Max *messages* (224) >> mean (38) + 3 * std (36) = 155 <br>\n",
    "Max *mb_used* (49745) >> mean (17207) + 3 * std (7570) = 39917 <br>\n",
    "That means that there are some outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outliers are only 2.9 % of the whole dataset\n"
     ]
    }
   ],
   "source": [
    "# I will proceed further original dataset, but later I will try to compare results with this cut dataset\n",
    "df_cut = df.query('calls < 162 and minutes < 1146 and messages < 155 and mb_used < 39917')\n",
    "print('Outliers are only {:.2} % of the whole dataset'.format((len(df)-len(df_cut))/len(df)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PHASE II. Split the source data into a training set, a validation set, and a test set.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PHASE II. \n",
    "**Split the source data into a training set, a validation set, and a test set.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test set doesn't exist. In that case, the source data has to be split into three parts: training, validation, and test. The sizes of validation set and test set are usually equal. It gives us source data split in a 3:1:1 ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as train_test_split divide dataset into two sets, to split it into three sets I should use it twice\n",
    "df_train, df_rest = train_test_split(df, test_size=0.4, random_state=22)\n",
    "df_valid, df_test = train_test_split(df_rest, test_size=0.5, random_state=23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of train set: 1928\n",
      "Size of validation set: 643\n",
      "Size of test set: 643\n",
      "1928 : 643 : 643 rate to each other as 3 : 1 : 1 as should be\n"
     ]
    }
   ],
   "source": [
    "print('Size of train set:', len(df_train))\n",
    "print('Size of validation set:', len(df_valid))\n",
    "print('Size of test set:', len(df_test))\n",
    "print(len(df_train),':',len(df_valid),':',len(df_test),'rate to each other as 3 : 1 : 1 as should be')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(df_test.is_ultra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_cut analysis (in parallel)\n",
    "df_cut_train, df_cut_rest = train_test_split(df_cut, test_size=0.4, random_state=22)\n",
    "df_cut_valid, df_cut_test = train_test_split(df_cut_rest, test_size=0.5, random_state=23)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PHASE III.  Investigate the quality of different models by changing hyperparameters. Briefly describe the findings of the study.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Firstly**, it should be noted that we have a problem of **classification**. <br>\n",
    "That means that we can use (1) **DecisionTreeClassifier**, (2) **Random Forest** and (3) **Logistic Regression** for this problem now. <br>\n",
    "**Secondly** it is necessary to define features and target. Target is a *is_ultra*; features all columns except for *is_ultra* <br>\n",
    "**Thirdly** target in this dataset is close to be balanced 603 (1-value) vs 1325 (0-value). That's why we use accuracy as a metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1) **DecisionTreeClassifier**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features and target for:\n",
    "# train set\n",
    "features = df_train.drop('is_ultra', axis=1)\n",
    "target = df_train['is_ultra']\n",
    "\n",
    "# validation set\n",
    "valid_features = df_valid.drop('is_ultra', axis=1)\n",
    "valid_target = df_valid['is_ultra']\n",
    "\n",
    "# test\n",
    "test_features = df_test.drop('is_ultra', axis=1)\n",
    "test_target = df_test['is_ultra']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_depth: 1\n",
      "Train set accuracy: 74.53%\n",
      "Validation set accuracy: 77.29%\n",
      "max_depth: 2\n",
      "Train set accuracy: 78.06%\n",
      "Validation set accuracy: 79.47%\n",
      "max_depth: 3\n",
      "Train set accuracy: 79.46%\n",
      "Validation set accuracy: 81.18%\n",
      "max_depth: 4\n",
      "Train set accuracy: 80.55%\n",
      "Validation set accuracy: 80.72%\n",
      "max_depth: 5\n",
      "Train set accuracy: 81.07%\n",
      "Validation set accuracy: 80.72%\n",
      "max_depth: 6\n",
      "Train set accuracy: 82.42%\n",
      "Validation set accuracy: 79.63%\n"
     ]
    }
   ],
   "source": [
    "# machine learning and accuracy of DecisionTreeClassifier\n",
    "for depth in range(1,7):    \n",
    "    model = DecisionTreeClassifier(max_depth=depth,random_state=22)\n",
    "    model.fit(features,target)\n",
    "    prediction = model.predict(features)\n",
    "    print('max_depth:',depth)\n",
    "    print('Train set accuracy: {:.2%}'.format(accuracy_score(target, prediction)))\n",
    "    valid_prediction = model.predict(valid_features)\n",
    "    print('Validation set accuracy: {:.2%}'.format(accuracy_score(valid_target, valid_prediction)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**So...** <br>\n",
    "1,2,3 are underfitted models because train set accuracy less than validation set accuracy\n",
    "5,6 are overfitted.\n",
    "It is not obvious for **max_depth = 5**, cause delta is not so great (81.07 - 80.72 = 0.35; less than 0.5%). However **Decision tree classifier is tend to be overfitting with high max_depth**. <br>\n",
    "So, I remain only model with **max_depth = 4 and test it further**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy: 79.16%\n"
     ]
    }
   ],
   "source": [
    "model = DecisionTreeClassifier(max_depth=4, random_state=22)\n",
    "model.fit(features,target)\n",
    "test_prediction = model.predict(test_features)\n",
    "print('Test set accuracy: {:.2%}'.format(accuracy_score(test_target, test_prediction)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1) Results on **DecisionTreeClassifier**: <br>\n",
    "model: max_depth=4 <br>\n",
    "Train set accuracy: 80.55% <br>\n",
    "Validation set accuracy: 80.72% <br>\n",
    "Test set accuracy: 79.16% <br>\n",
    "Accuracy is higher than threshold. And it is good, cause it is not highly differ from valid and train."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2) **RandomForestClassifier**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_estimators: 1\n",
      "Train set accuracy: 89.52%\n",
      "Validation set accuracy: 73.41%\n",
      "n_estimators: 6\n",
      "Train set accuracy: 96.01%\n",
      "Validation set accuracy: 78.85%\n",
      "n_estimators: 11\n",
      "Train set accuracy: 97.93%\n",
      "Validation set accuracy: 79.16%\n",
      "n_estimators: 16\n",
      "Train set accuracy: 98.44%\n",
      "Validation set accuracy: 79.47%\n",
      "n_estimators: 21\n",
      "Train set accuracy: 99.38%\n",
      "Validation set accuracy: 80.25%\n",
      "n_estimators: 26\n",
      "Train set accuracy: 99.43%\n",
      "Validation set accuracy: 80.09%\n",
      "n_estimators: 31\n",
      "Train set accuracy: 99.84%\n",
      "Validation set accuracy: 80.40%\n",
      "n_estimators: 36\n",
      "Train set accuracy: 99.84%\n",
      "Validation set accuracy: 79.94%\n",
      "n_estimators: 41\n",
      "Train set accuracy: 99.90%\n",
      "Validation set accuracy: 79.94%\n",
      "n_estimators: 46\n",
      "Train set accuracy: 99.79%\n",
      "Validation set accuracy: 79.63%\n"
     ]
    }
   ],
   "source": [
    "for now_estim in range(1,50,5):\n",
    "    model = RandomForestClassifier(n_estimators=now_estim, random_state=22)\n",
    "    model.fit(features,target)\n",
    "    prediction = model.predict(features)\n",
    "    print('n_estimators:',now_estim)\n",
    "    print('Train set accuracy: {:.2%}'.format(accuracy_score(target, prediction)))\n",
    "    valid_prediction = model.predict(valid_features)\n",
    "    print('Validation set accuracy: {:.2%}'.format(accuracy_score(valid_target, valid_prediction)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**So...(a)** <br>\n",
    "Accuracy of random forest model (applied on train test) generally rapidly increase with increase of n_estimators. <br>\n",
    "After n_estimators=21 accuracy doesn't change significantly. <br>\n",
    "We know that random forest doesn't tend to overfitting but we shouldn't use too high values. <br>\n",
    "I will remain n_estimators=20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_depth: 1\n",
      "Train set accuracy: 74.59%\n",
      "Validation set accuracy: 76.52%\n",
      "max_depth: 6\n",
      "Train set accuracy: 82.52%\n",
      "Validation set accuracy: 80.09%\n",
      "max_depth: 11\n",
      "Train set accuracy: 88.90%\n",
      "Validation set accuracy: 80.72%\n",
      "max_depth: 16\n",
      "Train set accuracy: 95.38%\n",
      "Validation set accuracy: 79.47%\n",
      "max_depth: 21\n",
      "Train set accuracy: 98.96%\n",
      "Validation set accuracy: 79.47%\n",
      "max_depth: 26\n",
      "Train set accuracy: 99.12%\n",
      "Validation set accuracy: 80.09%\n",
      "max_depth: 31\n",
      "Train set accuracy: 99.12%\n",
      "Validation set accuracy: 80.09%\n",
      "max_depth: 36\n",
      "Train set accuracy: 99.12%\n",
      "Validation set accuracy: 80.09%\n",
      "max_depth: 41\n",
      "Train set accuracy: 99.12%\n",
      "Validation set accuracy: 80.09%\n",
      "max_depth: 46\n",
      "Train set accuracy: 99.12%\n",
      "Validation set accuracy: 80.09%\n"
     ]
    }
   ],
   "source": [
    "for now_depth in range(1,50,5):\n",
    "    model = RandomForestClassifier(n_estimators=20, max_depth=now_depth, random_state=22)\n",
    "    model.fit(features,target)\n",
    "    prediction = model.predict(features)\n",
    "    print('max_depth:',now_depth)\n",
    "    print('Train set accuracy: {:.2%}'.format(accuracy_score(target, prediction)))\n",
    "    valid_prediction = model.predict(valid_features)\n",
    "    print('Validation set accuracy: {:.2%}'.format(accuracy_score(valid_target, valid_prediction)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**So...(b)** <br>\n",
    "default *max_depth* works good. It isn't necessary to use this hyperparameter. <br>\n",
    "I don't understand **WHY** train set accuracy much higher than validation set accuracy. If it was Decision tree Classifier I certainly claim that it is overfitting. <br>\n",
    "But maybe it is OK..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy: 79.63%\n"
     ]
    }
   ],
   "source": [
    "model = RandomForestClassifier(n_estimators=21, random_state=22)\n",
    "model.fit(features,target)\n",
    "print('Test set accuracy: {:.2%}'.format(model.score(test_features,test_target)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2) Results on **RandomForestClassifier**: <br>\n",
    "model: n_estimators=21 <br>\n",
    "Train set accuracy: 99.38% <br>\n",
    "Validation set accuracy: 80.25% <br>\n",
    "Test set accuracy: 79.63% <br>\n",
    "Accuracy is higher than threshold. Accuracy on validation and test set are the same. It is good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(3) **LogisticRegression**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set accuracy: 70.80%\n",
      "Validation set accuracy: 73.25%\n",
      "Test set accuracy: 70.30%\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(solver='liblinear', random_state=25) # random_state is here to shuffle the data\n",
    "model.fit(features,target)\n",
    "print('Train set accuracy: {:.2%}'.format(model.score(features,target)))\n",
    "print('Validation set accuracy: {:.2%}'.format(model.score(valid_features,valid_target)))\n",
    "print('Test set accuracy: {:.2%}'.format(model.score(test_features,test_target)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set accuracy: 70.80%\n",
      "Validation set accuracy: 73.41%\n",
      "Test set accuracy: 70.30%\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(solver='lbfgs')\n",
    "model.fit(features,target)\n",
    "print('Train set accuracy: {:.2%}'.format(model.score(features,target)))\n",
    "print('Validation set accuracy: {:.2%}'.format(model.score(valid_features,valid_target)))\n",
    "print('Test set accuracy: {:.2%}'.format(model.score(test_features,test_target)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(3) Results on **LogisticRegression**: <br>\n",
    "model of logistic regression is \"linear\" so there is no any randomness here. <br>\n",
    "Train set accuracy: 70.80% <br>\n",
    "Validation set accuracy: 73.25% <br>\n",
    "Test set accuracy: 70.30% <br>\n",
    "Accuracy is lower than threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PHASE IV.  Additional task: sanity check the model. This data is more complex than what you’re used to working with, so it's not an easy task. We'll take a closer look at it later.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sanity check** <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ratio of 1-value is: 30.6%\n",
      "Ratio of 0-value is: 69.4%\n"
     ]
    }
   ],
   "source": [
    "print('Ratio of 1-value is: {:.1%}'.format(sum(df['is_ultra']) / len(df['is_ultra'])))\n",
    "print('Ratio of 0-value is: {:.1%}'.format( 1 - sum(df['is_ultra']) / len(df['is_ultra'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**So...** <br>\n",
    "The \"sanity\"-accuracy is 69.4% because we can assign all data to 0-value. <br>\n",
    "Our models predict better than this \"0-value\" model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PHASE V (additional) Comparison of results of different model:**\n",
    "**Recover our results**\n",
    "(1) Results on **DecisionTreeClassifier**: <br>\n",
    "model: max_depth=4 <br>\n",
    "Train set accuracy: 80.55% <br>\n",
    "Validation set accuracy: 80.72% <br>\n",
    "Test set accuracy: 79.16% <br>\n",
    "Accuracy is higher than threshold. And it is good, cause it is not highly differ from valid and train.\n",
    "\n",
    "(2) Results on **RandomForestClassifier**: <br>\n",
    "model: n_estimators=21 <br>\n",
    "Train set accuracy: 99.38% <br>\n",
    "Validation set accuracy: 80.25% <br>\n",
    "Test set accuracy: 79.63% <br>\n",
    "Accuracy is higher than threshold. Accuracy on validation and test set are the same. It is good.\n",
    "\n",
    "(3) Results on **LogisticRegression**: <br>\n",
    "model of logistic regression is \"linear\" so there is no any randomness here. <br>\n",
    "Train set accuracy: 70.80% <br>\n",
    "Validation set accuracy: 73.25% <br>\n",
    "Test set accuracy: 70.30% <br>\n",
    "Accuracy is lower than threshold.\n",
    "\n",
    "Although **RandomForestClassifier** accuracy on test set is a little bit better than **DecisionTreeClassifier** (79.63% vs 79.16%). It seems one can use both of this models to predict consumers choice. Mobile carrier Megaline can offer to its clients one of this plans Smart or Ultra with a good (in fact not bad) accuracy 80%. It is better than 70%, which we can obtain in sanity check case. Of course one should make our model better. How? <br>\n",
    "- filter outliers\n",
    "- create some new features (maybe try something like sum of minutes(normalized) and messages(normalized))\n",
    "- enter in our dataset data, which naturally should be in a company as a gender of a client, age and others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "Finally, we created a model which can predict clients behavior and needs. <br>\n",
    "We compared 3 models: RandomForestClassifier, LogisticRegression and DecisionTreeClassifier. Two of them (trees-based) gives results better than threshold (75%). <br>\n",
    "We recommend DecisionTreeClassifier because RandomForestClassifier gives strange results on train set as the model is overfitting one. <br>\n",
    "We give recommendations how to improve our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PostScriptum\n",
    "It was interesting to apply our model to dataframe without outliers. Maybe it will give better results. <br>\n",
    "And it is not so time-consuming because we prepared all necessary data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set accuracy: 80.83%\n",
      "Validation set accuracy: 77.88%\n",
      "Test set accuracy: 78.08%\n"
     ]
    }
   ],
   "source": [
    "# features and target for:\n",
    "# train set\n",
    "features = df_cut_train.drop('is_ultra', axis=1)\n",
    "target = df_cut_train['is_ultra']\n",
    "\n",
    "# validation set\n",
    "valid_features = df_cut_valid.drop('is_ultra', axis=1)\n",
    "valid_target = df_cut_valid['is_ultra']\n",
    "\n",
    "# test\n",
    "test_features = df_cut_test.drop('is_ultra', axis=1)\n",
    "test_target = df_cut_test['is_ultra']\n",
    "\n",
    "model = DecisionTreeClassifier(max_depth=4, random_state=22)\n",
    "model.fit(features,target)\n",
    "print('Train set accuracy: {:.2%}'.format(model.score(features,target)))\n",
    "print('Validation set accuracy: {:.2%}'.format(model.score(valid_features,valid_target)))\n",
    "print('Test set accuracy: {:.2%}'.format(model.score(test_features,test_target)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BUT our previous results were better! <br>\n",
    "(1) Results on **DecisionTreeClassifier**: <br>\n",
    "model: max_depth=4 <br>\n",
    "Train set accuracy: 80.55% <br>\n",
    "Validation set accuracy: 80.72% <br>\n",
    "Test set accuracy: 79.16% <br>\n",
    "\n",
    "Maybe it is because all of this outliers easy to predict? If someone use ~50Gb of data he without doubts should be recommended to use Ultra plan. Maybe our model (our black box) take this into account.\n",
    "\n",
    "I am not satisfied with the quality of the prediction. But is OK for now."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
